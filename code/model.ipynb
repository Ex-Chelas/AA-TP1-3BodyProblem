{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Load the data set\n",
    "2. Display the first rows of the data set"
   ],
   "id": "b29321ca9e3f982b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T00:16:35.324425Z",
     "start_time": "2024-10-09T00:16:32.225332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data_frame(path):\n",
    "    \"\"\"\n",
    "    Load data set from file path\n",
    "    :param path: the path of the file to turn into a data set\n",
    "    :return: a created data set based on the header of the file\n",
    "    \"\"\"\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "df_train = load_data_frame('../data/X_train.csv')\n",
    "print(df_train.head())"
   ],
   "id": "eea591ee5bdcd774",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          t       x_1       y_1     v_x_1     v_y_1       x_2       y_2  \\\n",
      "0  0.000000  1.000000  0.000000  0.000000  0.000000 -0.266467  0.859196   \n",
      "1  0.039062  0.999548  0.000092 -0.023159  0.004731 -0.266261  0.858781   \n",
      "2  0.078125  0.998190  0.000370 -0.046362  0.009474 -0.265641  0.857535   \n",
      "3  0.117188  0.995925  0.000833 -0.069654  0.014239 -0.264606  0.855456   \n",
      "4  0.156250  0.992747  0.001483 -0.093080  0.019040 -0.263154  0.852540   \n",
      "\n",
      "      v_x_2     v_y_2       x_3       y_3     v_x_3     v_y_3  Id  \n",
      "0  0.000000  0.000000 -0.733533 -0.859196  0.000000  0.000000   0  \n",
      "1  0.010574 -0.021257 -0.733287 -0.858874  0.012584  0.016526   1  \n",
      "2  0.021172 -0.042552 -0.732549 -0.857905  0.025189  0.033078   2  \n",
      "3  0.031817 -0.063924 -0.731318 -0.856289  0.037837  0.049685   3  \n",
      "4  0.042533 -0.085412 -0.729592 -0.854022  0.050548  0.066372   4  \n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then we sanitize the data set by removing the columns that are not useful for the model. We also remove the rows with missing values.",
   "id": "8a6079167e990a95"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-09T00:16:35.855119Z",
     "start_time": "2024-10-09T00:16:35.353948Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "def sanitize_data_set(data_frame):\n",
    "    \"\"\"\n",
    "    Sanitize the data set by removing the rows that are not useful for the model (Nan values, etc.)\n",
    "    :param data_frame: the data set to sanitize\n",
    "    :return: the sanitized data set\n",
    "    \"\"\"\n",
    "    ## data_set = data_set.drop(['Id'], axis=1) # we might wanna merge train and test data\n",
    "    data_frame = data_frame.replace(0, np.nan)\n",
    "    data_frame = data_frame.dropna(axis=0, how='all')\n",
    "    data_frame = data_frame.replace(np.nan, 0)\n",
    "    return data_frame\n",
    "\n",
    "df_train = sanitize_data_set(df_train)\n",
    "print(df_train.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          t       x_1       y_1     v_x_1     v_y_1       x_2       y_2  \\\n",
      "0  0.000000  1.000000  0.000000  0.000000  0.000000 -0.266467  0.859196   \n",
      "1  0.039062  0.999548  0.000092 -0.023159  0.004731 -0.266261  0.858781   \n",
      "2  0.078125  0.998190  0.000370 -0.046362  0.009474 -0.265641  0.857535   \n",
      "3  0.117188  0.995925  0.000833 -0.069654  0.014239 -0.264606  0.855456   \n",
      "4  0.156250  0.992747  0.001483 -0.093080  0.019040 -0.263154  0.852540   \n",
      "\n",
      "      v_x_2     v_y_2       x_3       y_3     v_x_3     v_y_3   Id  \n",
      "0  0.000000  0.000000 -0.733533 -0.859196  0.000000  0.000000  0.0  \n",
      "1  0.010574 -0.021257 -0.733287 -0.858874  0.012584  0.016526  1.0  \n",
      "2  0.021172 -0.042552 -0.732549 -0.857905  0.025189  0.033078  2.0  \n",
      "3  0.031817 -0.063924 -0.731318 -0.856289  0.037837  0.049685  3.0  \n",
      "4  0.042533 -0.085412 -0.729592 -0.854022  0.050548  0.066372  4.0  \n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we do the same for the target data set.",
   "id": "23a4bd091eadf25"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T00:16:37.185017Z",
     "start_time": "2024-10-09T00:16:35.887933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_test = load_data_frame('../data/X_test.csv')\n",
    "df_test = sanitize_data_set(df_test)\n",
    "print(df_test.head())"
   ],
   "id": "5ec06c4d7a4c4c92",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Id         t  x0_1  y0_1      x0_2      y0_2      x0_3      y0_3\n",
      "0  0.0  0.000000   1.0   0.0 -0.179617  0.730085 -0.820383 -0.730085\n",
      "1  1.0  0.039062   1.0   0.0 -0.179617  0.730085 -0.820383 -0.730085\n",
      "2  2.0  0.078125   1.0   0.0 -0.179617  0.730085 -0.820383 -0.730085\n",
      "3  3.0  0.117188   1.0   0.0 -0.179617  0.730085 -0.820383 -0.730085\n",
      "4  4.0  0.156250   1.0   0.0 -0.179617  0.730085 -0.820383 -0.730085\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we merge the data frames by their 'Id' column and drop all columns with only zeros, or if the 'Id's are not in both data frames.",
   "id": "ad3b4941a1e78a31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T00:16:38.248683Z",
     "start_time": "2024-10-09T00:16:37.216957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def merge_data_frames(data_frame, target_data_frame):\n",
    "    \"\"\"\n",
    "    Merge two data frames by their 'Id' column\n",
    "    :param data_frame: the first data frame\n",
    "    :param target_data_frame: the second data frame\n",
    "    :return: the merged data frame\n",
    "    \"\"\"\n",
    "    merged_data_frame = pd.merge(data_frame, target_data_frame, how='inner', on=['Id','t'])\n",
    "    return merged_data_frame\n",
    "\n",
    "merged_df = merge_data_frames(df_train, df_test)\n",
    "merged_df = sanitize_data_set(merged_df)\n",
    "print(merged_df.head())"
   ],
   "id": "da78c973d45ec1c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          t       x_1       y_1     v_x_1     v_y_1       x_2       y_2  \\\n",
      "0  0.000000  1.000000  0.000000  0.000000  0.000000 -0.266467  0.859196   \n",
      "1  0.039062  0.999548  0.000092 -0.023159  0.004731 -0.266261  0.858781   \n",
      "2  0.078125  0.998190  0.000370 -0.046362  0.009474 -0.265641  0.857535   \n",
      "3  0.117188  0.995925  0.000833 -0.069654  0.014239 -0.264606  0.855456   \n",
      "4  0.156250  0.992747  0.001483 -0.093080  0.019040 -0.263154  0.852540   \n",
      "\n",
      "      v_x_2     v_y_2       x_3       y_3     v_x_3     v_y_3   Id  x0_1  \\\n",
      "0  0.000000  0.000000 -0.733533 -0.859196  0.000000  0.000000  0.0   1.0   \n",
      "1  0.010574 -0.021257 -0.733287 -0.858874  0.012584  0.016526  1.0   1.0   \n",
      "2  0.021172 -0.042552 -0.732549 -0.857905  0.025189  0.033078  2.0   1.0   \n",
      "3  0.031817 -0.063924 -0.731318 -0.856289  0.037837  0.049685  3.0   1.0   \n",
      "4  0.042533 -0.085412 -0.729592 -0.854022  0.050548  0.066372  4.0   1.0   \n",
      "\n",
      "   y0_1      x0_2      y0_2      x0_3      y0_3  \n",
      "0   0.0 -0.179617  0.730085 -0.820383 -0.730085  \n",
      "1   0.0 -0.179617  0.730085 -0.820383 -0.730085  \n",
      "2   0.0 -0.179617  0.730085 -0.820383 -0.730085  \n",
      "3   0.0 -0.179617  0.730085 -0.820383 -0.730085  \n",
      "4   0.0 -0.179617  0.730085 -0.820383 -0.730085  \n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we split the data set into the features and the target, by adding also the 'Id' column to both data frames, as it was before.",
   "id": "70719f03752611a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T00:16:38.434039Z",
     "start_time": "2024-10-09T00:16:38.292493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_data_frame(data_frame, target_columns):\n",
    "    features = data_frame.drop(columns=target_columns[1:], axis=1)\n",
    "    features['Id'] = features.index  # Add the Id column\n",
    "\n",
    "    # Create a full copy of the target data to avoid the warning\n",
    "    target = data_frame[target_columns].copy()\n",
    "    target['Id'] = target.index  # Add the Id column to the target data\n",
    "\n",
    "    return features, target\n",
    "\n",
    "# Assuming `merged_df` is the data frame you're splitting\n",
    "df_train, df_test = split_data_frame(merged_df, ['t','x0_1', 'y0_1', 'x0_2', 'y0_2', 'x0_3', 'y0_3'])\n",
    "\n",
    "print(df_train.columns)\n",
    "print(df_test.columns)"
   ],
   "id": "24f229f4539a6607",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['t', 'x_1', 'y_1', 'v_x_1', 'v_y_1', 'x_2', 'y_2', 'v_x_2', 'v_y_2',\n",
      "       'x_3', 'y_3', 'v_x_3', 'v_y_3', 'Id'],\n",
      "      dtype='object')\n",
      "Index(['t', 'x0_1', 'y0_1', 'x0_2', 'y0_2', 'x0_3', 'y0_3', 'Id'], dtype='object')\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Learn a baseline model that can predict the position of each of the 3 bodies at a given time t, given a set of initial conditions. Your baseline will be a Linear Regression model.\n",
    "For the baseline model, make a pipeline and add a StandardScaler instance before the regressor. See the pipeline tutorial on the Tutorials document for the course."
   ],
   "id": "bfc47323174bb520"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T00:30:02.661192Z",
     "start_time": "2024-10-09T00:29:58.194180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Features: Include initial positions, velocities, and time 't'\n",
    "train_columns = ['t', \n",
    "                       'x_1', 'y_1', 'v_x_1', 'v_y_1', \n",
    "                       'x_2', 'y_2', 'v_x_2', 'v_y_2', \n",
    "                       'x_3', 'y_3', 'v_x_3', 'v_y_3'\n",
    "                       ]\n",
    "\n",
    "# Targets: Predict future positions (x0_1, y0_1, x0_2, y0_2, x0_3, y0_3)\n",
    "test_columns = ['Id', 't', 'x_1', 'y_1', 'x_2', 'y_2', 'x_3', 'y_3']\n",
    "\n",
    "def linear_predict_positions():\n",
    "    \n",
    "    # Split your dataset into features (X) and targets (y)\n",
    "    # x = df_train[train_columns]  # Using positions, velocities, and time 't' as features\n",
    "    # y = df_test[test_columns]     # Predicting future positions for a given time\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    x_train, x_validate, y_train, y_validate = train_test_split(df_train, df_test, test_size=0.2)\n",
    "    \n",
    "    # Create a pipeline with scaling and linear regression\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Standardize the features\n",
    "        ('regressor', LinearRegression())  # Apply Linear Regression\n",
    "    ])\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    pipeline.fit(x_train, y_train)\n",
    "    \n",
    "    # Predict positions (x0_1, y0_1, x0_2, y0_2, x0_3, y0_3) on the test data\n",
    "    y_pred = pipeline.predict(x_validate)\n",
    "    \n",
    "    #validate model\n",
    "    score = pipeline.score(x_validate, y_validate)\n",
    "    print(\"Model score: \", score)\n",
    "    \n",
    "    # Create a DataFrame with the predicted values\n",
    "    predictions_df = pd.DataFrame(y_pred, columns=test_columns)\n",
    "    \n",
    "    # Add an 'Id' column if required by the competition (use the index or some unique identifier)\n",
    "    predictions_df['Id'] = np.arange(1, len(predictions_df) + 1)  # Assuming Ids start from 1\n",
    "    predictions_df = predictions_df.drop(columns=['t'])\n",
    "\n",
    "    # Save the predictions to a CSV file in the required format\n",
    "    predictions_df.to_csv('baseline-model-positions-with-time.csv', index=False)\n",
    "    print(\"Predictions for the positions saved successfully with time!\")\n",
    "    return predictions_df\n",
    "\n",
    "# Call the function\n",
    "predictions_df = linear_predict_positions()\n"
   ],
   "id": "5e6566ae27cfd74e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score:  0.5009397124826979\n",
      "Predictions for the positions saved successfully with time!\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f5b17cf5e66782f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T00:16:40.553310600Z",
     "start_time": "2024-10-06T08:32:45.888364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_y_yhat(y_test, y_pred, plot_title=\"plot\"):\n",
    "    # Ensure y_test and y_pred are NumPy arrays\n",
    "    y_test = np.array(y_test)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Column labels for positions\n",
    "    labels = ['x_1', 'y_1', 'x_2', 'y_2', 'x_3', 'y_3']\n",
    "    \n",
    "    # Maximum number of points to plot\n",
    "    MAX = min(500, len(y_test))  # Ensure MAX is not larger than y_test length\n",
    "    \n",
    "    # Select random indices, ensuring they are within bounds\n",
    "    idx = np.random.choice(len(y_test), MAX, replace=False)\n",
    "\n",
    "    # Create a 10x10 figure\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Plot each of the 6 position variables (x_1, y_1, x_2, y_2, x_3, y_3)\n",
    "    for i in range(6):\n",
    "        x0 = np.min([np.min(y_test[idx, i]), np.min(y_pred[idx, i])])  # Min value for the diagonal\n",
    "        x1 = np.max([np.max(y_test[idx, i]), np.max(y_pred[idx, i])])  # Max value for the diagonal\n",
    "        \n",
    "        plt.subplot(3, 2, i+1)\n",
    "        plt.scatter(y_test[idx, i], y_pred[idx, i], alpha=0.5)  # Scatter plot\n",
    "        plt.xlabel('True ' + labels[i])\n",
    "        plt.ylabel('Predicted ' + labels[i])\n",
    "        plt.plot([x0, x1], [x0, x1], color='red')  # Plot diagonal for reference\n",
    "        plt.axis('square')\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(plot_title + '.pdf')\n",
    "    plt.show()\n",
    "\n",
    "# Assuming df_target is the true values, and predictions_df is the predicted values\n",
    "plot_y_yhat(df_test, predictions_df)"
   ],
   "id": "93afdefda881552c",
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 876334 is out of bounds for axis 0 with size 176968",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 39\u001B[0m\n\u001B[0;32m     36\u001B[0m     plt\u001B[38;5;241m.\u001B[39mshow()\n\u001B[0;32m     38\u001B[0m \u001B[38;5;66;03m# Assuming df_target is the true values, and predictions_df is the predicted values\u001B[39;00m\n\u001B[1;32m---> 39\u001B[0m \u001B[43mplot_y_yhat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_target\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpredictions_df\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[34], line 24\u001B[0m, in \u001B[0;36mplot_y_yhat\u001B[1;34m(y_test, y_pred, plot_title)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# Plot each of the 6 position variables (x_1, y_1, x_2, y_2, x_3, y_3)\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m6\u001B[39m):\n\u001B[1;32m---> 24\u001B[0m     x0 \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmin([np\u001B[38;5;241m.\u001B[39mmin(y_test[idx, i]), np\u001B[38;5;241m.\u001B[39mmin(\u001B[43my_pred\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m)])  \u001B[38;5;66;03m# Min value for the diagonal\u001B[39;00m\n\u001B[0;32m     25\u001B[0m     x1 \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmax([np\u001B[38;5;241m.\u001B[39mmax(y_test[idx, i]), np\u001B[38;5;241m.\u001B[39mmax(y_pred[idx, i])])  \u001B[38;5;66;03m# Max value for the diagonal\u001B[39;00m\n\u001B[0;32m     27\u001B[0m     plt\u001B[38;5;241m.\u001B[39msubplot(\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m2\u001B[39m, i\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mIndexError\u001B[0m: index 876334 is out of bounds for axis 0 with size 176968"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 34
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
