{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Load the data set\n",
    "2. Display the first rows of the data set"
   ],
   "id": "b29321ca9e3f982b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:10:13.255999Z",
     "start_time": "2024-09-28T16:10:10.852448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data_frame(path):\n",
    "    \"\"\"\n",
    "    Load data set from file path\n",
    "    :param path: the path of the file to turn into a data set\n",
    "    :return: a created data set based on the header of the file\n",
    "    \"\"\"\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "df = load_data_frame('../data/X_train.csv')\n",
    "print(df.head())"
   ],
   "id": "eea591ee5bdcd774",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          t       x_1       y_1     v_x_1     v_y_1       x_2       y_2  \\\n",
      "0  0.000000  1.000000  0.000000  0.000000  0.000000 -0.266467  0.859196   \n",
      "1  0.039062  0.999548  0.000092 -0.023159  0.004731 -0.266261  0.858781   \n",
      "2  0.078125  0.998190  0.000370 -0.046362  0.009474 -0.265641  0.857535   \n",
      "3  0.117188  0.995925  0.000833 -0.069654  0.014239 -0.264606  0.855456   \n",
      "4  0.156250  0.992747  0.001483 -0.093080  0.019040 -0.263154  0.852540   \n",
      "\n",
      "      v_x_2     v_y_2       x_3       y_3     v_x_3     v_y_3  Id  \n",
      "0  0.000000  0.000000 -0.733533 -0.859196  0.000000  0.000000   0  \n",
      "1  0.010574 -0.021257 -0.733287 -0.858874  0.012584  0.016526   1  \n",
      "2  0.021172 -0.042552 -0.732549 -0.857905  0.025189  0.033078   2  \n",
      "3  0.031817 -0.063924 -0.731318 -0.856289  0.037837  0.049685   3  \n",
      "4  0.042533 -0.085412 -0.729592 -0.854022  0.050548  0.066372   4  \n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then we sanitize the data set by removing the columns that are not useful for the model. We also remove the rows with missing values.",
   "id": "8a6079167e990a95"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-28T16:10:13.715560Z",
     "start_time": "2024-09-28T16:10:13.279004Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "def sanitize_data_set(data_frame):\n",
    "    \"\"\"\n",
    "    Sanitize the data set by removing the rows that are not useful for the model (Nan values, etc.)\n",
    "    :param data_frame: the data set to sanitize\n",
    "    :return: the sanitized data set\n",
    "    \"\"\"\n",
    "    ## data_set = data_set.drop(['Id'], axis=1) # we might wanna merge train and test data\n",
    "    data_frame = data_frame.replace(0, np.nan)\n",
    "    data_frame = data_frame.dropna(axis=0, how='all')\n",
    "    data_frame = data_frame.replace(np.nan, 0)\n",
    "    return data_frame\n",
    "\n",
    "df = sanitize_data_set(df)\n",
    "print(df.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          t       x_1       y_1     v_x_1     v_y_1       x_2       y_2  \\\n",
      "0  0.000000  1.000000  0.000000  0.000000  0.000000 -0.266467  0.859196   \n",
      "1  0.039062  0.999548  0.000092 -0.023159  0.004731 -0.266261  0.858781   \n",
      "2  0.078125  0.998190  0.000370 -0.046362  0.009474 -0.265641  0.857535   \n",
      "3  0.117188  0.995925  0.000833 -0.069654  0.014239 -0.264606  0.855456   \n",
      "4  0.156250  0.992747  0.001483 -0.093080  0.019040 -0.263154  0.852540   \n",
      "\n",
      "      v_x_2     v_y_2       x_3       y_3     v_x_3     v_y_3   Id  \n",
      "0  0.000000  0.000000 -0.733533 -0.859196  0.000000  0.000000  0.0  \n",
      "1  0.010574 -0.021257 -0.733287 -0.858874  0.012584  0.016526  1.0  \n",
      "2  0.021172 -0.042552 -0.732549 -0.857905  0.025189  0.033078  2.0  \n",
      "3  0.031817 -0.063924 -0.731318 -0.856289  0.037837  0.049685  3.0  \n",
      "4  0.042533 -0.085412 -0.729592 -0.854022  0.050548  0.066372  4.0  \n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we do the same for the target data set.",
   "id": "23a4bd091eadf25"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:10:14.857203Z",
     "start_time": "2024-09-28T16:10:13.840566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_df = load_data_frame('../data/X_test.csv')\n",
    "target_df = sanitize_data_set(target_df)\n",
    "print(target_df.head())"
   ],
   "id": "5ec06c4d7a4c4c92",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Id         t  x0_1  y0_1      x0_2      y0_2      x0_3      y0_3\n",
      "0  0.0  0.000000   1.0   0.0 -0.179617  0.730085 -0.820383 -0.730085\n",
      "1  1.0  0.039062   1.0   0.0 -0.179617  0.730085 -0.820383 -0.730085\n",
      "2  2.0  0.078125   1.0   0.0 -0.179617  0.730085 -0.820383 -0.730085\n",
      "3  3.0  0.117188   1.0   0.0 -0.179617  0.730085 -0.820383 -0.730085\n",
      "4  4.0  0.156250   1.0   0.0 -0.179617  0.730085 -0.820383 -0.730085\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we merge the data frames by their 'Id' column and drop all columns with only zeros, or if the 'Id's are not in both data frames.",
   "id": "ad3b4941a1e78a31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:10:15.691714Z",
     "start_time": "2024-09-28T16:10:14.967561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def merge_data_frames(data_frame, target_data_frame):\n",
    "    \"\"\"\n",
    "    Merge two data frames by their 'Id' column\n",
    "    :param data_frame: the first data frame\n",
    "    :param target_data_frame: the second data frame\n",
    "    :return: the merged data frame\n",
    "    \"\"\"\n",
    "    merged_data_frame = pd.merge(data_frame, target_data_frame, how='inner', on='Id')\n",
    "    return merged_data_frame\n",
    "\n",
    "merged_df = merge_data_frames(df, target_df)\n",
    "merged_df = sanitize_data_set(merged_df)\n",
    "print(merged_df.head())"
   ],
   "id": "da78c973d45ec1c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        t_x       x_1       y_1     v_x_1     v_y_1       x_2       y_2  \\\n",
      "0  0.000000  1.000000  0.000000  0.000000  0.000000 -0.266467  0.859196   \n",
      "1  0.039062  0.999548  0.000092 -0.023159  0.004731 -0.266261  0.858781   \n",
      "2  0.078125  0.998190  0.000370 -0.046362  0.009474 -0.265641  0.857535   \n",
      "3  0.117188  0.995925  0.000833 -0.069654  0.014239 -0.264606  0.855456   \n",
      "4  0.156250  0.992747  0.001483 -0.093080  0.019040 -0.263154  0.852540   \n",
      "\n",
      "      v_x_2     v_y_2       x_3  ...     v_x_3     v_y_3   Id       t_y  x0_1  \\\n",
      "0  0.000000  0.000000 -0.733533  ...  0.000000  0.000000  0.0  0.000000   1.0   \n",
      "1  0.010574 -0.021257 -0.733287  ...  0.012584  0.016526  1.0  0.039062   1.0   \n",
      "2  0.021172 -0.042552 -0.732549  ...  0.025189  0.033078  2.0  0.078125   1.0   \n",
      "3  0.031817 -0.063924 -0.731318  ...  0.037837  0.049685  3.0  0.117188   1.0   \n",
      "4  0.042533 -0.085412 -0.729592  ...  0.050548  0.066372  4.0  0.156250   1.0   \n",
      "\n",
      "   y0_1      x0_2      y0_2      x0_3      y0_3  \n",
      "0   0.0 -0.179617  0.730085 -0.820383 -0.730085  \n",
      "1   0.0 -0.179617  0.730085 -0.820383 -0.730085  \n",
      "2   0.0 -0.179617  0.730085 -0.820383 -0.730085  \n",
      "3   0.0 -0.179617  0.730085 -0.820383 -0.730085  \n",
      "4   0.0 -0.179617  0.730085 -0.820383 -0.730085  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we split the data set into the features and the target, by adding also the 'Id' column to both data frames, as it was before.",
   "id": "70719f03752611a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:10:15.909208Z",
     "start_time": "2024-09-28T16:10:15.806968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_data_frame(data_frame, target_columns):\n",
    "    \"\"\"\n",
    "    Split the data frame into the features and the target\n",
    "    :param target_columns: the columns of the target data frame\n",
    "    :param data_frame: the data frame to split\n",
    "    :return: the features and the target\n",
    "    \"\"\"\n",
    "    features = data_frame.drop(columns=target_columns[1:], axis=1)\n",
    "    features['Id'] = features.index\n",
    "    target = data_frame[target_df.columns]\n",
    "    target['Id'] = target.index\n",
    "    return features, target\n",
    "print(merged_df.columns)\n",
    "target_column_names = target_df.columns[1:]\n",
    "df_features, df_target = split_data_frame(merged_df, target_column_names)"
   ],
   "id": "24f229f4539a6607",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['t_x', 'x_1', 'y_1', 'v_x_1', 'v_y_1', 'x_2', 'y_2', 'v_x_2', 'v_y_2',\n",
      "       'x_3', 'y_3', 'v_x_3', 'v_y_3', 'Id', 't_y', 'x0_1', 'y0_1', 'x0_2',\n",
      "       'y0_2', 'x0_3', 'y0_3'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['t'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[58], line 15\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(merged_df\u001B[38;5;241m.\u001B[39mcolumns)\n\u001B[0;32m     14\u001B[0m target_column_names \u001B[38;5;241m=\u001B[39m target_df\u001B[38;5;241m.\u001B[39mcolumns[\u001B[38;5;241m1\u001B[39m:]\n\u001B[1;32m---> 15\u001B[0m df_features, df_target \u001B[38;5;241m=\u001B[39m \u001B[43msplit_data_frame\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmerged_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_column_names\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[58], line 10\u001B[0m, in \u001B[0;36msplit_data_frame\u001B[1;34m(data_frame, target_columns)\u001B[0m\n\u001B[0;32m      8\u001B[0m features \u001B[38;5;241m=\u001B[39m data_frame\u001B[38;5;241m.\u001B[39mdrop(columns\u001B[38;5;241m=\u001B[39mtarget_columns[\u001B[38;5;241m1\u001B[39m:], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      9\u001B[0m features[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mId\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m features\u001B[38;5;241m.\u001B[39mindex\n\u001B[1;32m---> 10\u001B[0m target \u001B[38;5;241m=\u001B[39m \u001B[43mdata_frame\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtarget_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     11\u001B[0m target[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mId\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m target\u001B[38;5;241m.\u001B[39mindex\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m features, target\n",
      "File \u001B[1;32m~\\Desktop\\Universidade\\NOVA\\1Semester\\AA\\3BodyProblem\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   4106\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_iterator(key):\n\u001B[0;32m   4107\u001B[0m         key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(key)\n\u001B[1;32m-> 4108\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_indexer_strict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcolumns\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m   4110\u001B[0m \u001B[38;5;66;03m# take() does not accept boolean indexers\u001B[39;00m\n\u001B[0;32m   4111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(indexer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n",
      "File \u001B[1;32m~\\Desktop\\Universidade\\NOVA\\1Semester\\AA\\3BodyProblem\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001B[0m, in \u001B[0;36mIndex._get_indexer_strict\u001B[1;34m(self, key, axis_name)\u001B[0m\n\u001B[0;32m   6197\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   6198\u001B[0m     keyarr, indexer, new_indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reindex_non_unique(keyarr)\n\u001B[1;32m-> 6200\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raise_if_missing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeyarr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   6202\u001B[0m keyarr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtake(indexer)\n\u001B[0;32m   6203\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Index):\n\u001B[0;32m   6204\u001B[0m     \u001B[38;5;66;03m# GH 42790 - Preserve name from an Index\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\Universidade\\NOVA\\1Semester\\AA\\3BodyProblem\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001B[0m, in \u001B[0;36mIndex._raise_if_missing\u001B[1;34m(self, key, indexer, axis_name)\u001B[0m\n\u001B[0;32m   6249\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNone of [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] are in the [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maxis_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   6251\u001B[0m not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(ensure_index(key)[missing_mask\u001B[38;5;241m.\u001B[39mnonzero()[\u001B[38;5;241m0\u001B[39m]]\u001B[38;5;241m.\u001B[39munique())\n\u001B[1;32m-> 6252\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnot_found\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not in index\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mKeyError\u001B[0m: \"['t'] not in index\""
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TODO: Somehow i lost the column t that represented the time. to be fixed.",
   "id": "bfc47323174bb520"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
